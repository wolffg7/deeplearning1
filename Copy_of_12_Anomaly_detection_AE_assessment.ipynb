{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolffg7/deeplearning1/blob/main/Copy_of_12_Anomaly_detection_AE_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gEG4uwkDgAB"
      },
      "source": [
        "## Anomaly detection with autoencoders\n",
        "\n",
        "In this notebook we you have to create a fully-connected neural network based autoencoder in order to detect credit card frauds. In the dataset there are 492 labeled frauds and 284807 normal transactions. First, let's download the dataset and unpack it (original source of the dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud - before downloading the data please read and accept the licence of the original datasource):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5rudWIupy4i"
      },
      "source": [
        "!wget https://www.deeplearningoktatas.hu/downloads/Anomaly/creditcardfraud.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUpEkrcQyxj7"
      },
      "source": [
        "%%capture\n",
        "!unzip creditcardfraud.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwHyNENGEfd8"
      },
      "source": [
        "## Imports\n",
        "Import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqrc2eO1reot"
      },
      "source": [
        "import pandas as pd # \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn import preprocessing \n",
        "from sklearn.metrics import mean_squared_error\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Reading and preparing the data\n",
        "Read the creditcard.csv into a Pandas Dataframe with the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function. "
      ],
      "metadata": {
        "id": "1eDKL0n98aP-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWON-DgYrhFQ"
      },
      "source": [
        "df = <TODO>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIFxgcDDEk9l"
      },
      "source": [
        "As the next step, select all the columns of the dataframe for the input X, but the \"Class\" column. And select the \"Class\" column for output Y. Make sure that both X and Y are two dimensional Numpy arrays. \n",
        "\n",
        "Hint:\n",
        "* you can use df.loc to select rows and columns from a dataframe\n",
        "* you can get the Numpy array of a Pandas object with .values, eg. df['myfeature'].values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_1wNixrqRQ"
      },
      "source": [
        "X = <TODO>\n",
        "Y = <TODO>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X:\",X.shape)\n",
        "print(\"Shape of Y:\",Y.shape)"
      ],
      "metadata": {
        "id": "oxBkuXVk-zCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's standardize the data:"
      ],
      "metadata": {
        "id": "e89JTdnC9tBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X = scaler.transform(X)"
      ],
      "metadata": {
        "id": "SVfMPeon9v1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRBJka1AEp_A"
      },
      "source": [
        "## Exercise 2: defining the autoencoder\n",
        "Let's define a model according to the following instructions:\n",
        "* the model's inputs and outputs should be the same as the number of X's features (columns)\n",
        "* the model should have 5 hidden layers, with 16-10-6-10-16 neurons in the hidden layers, respectively\n",
        "* all layers but the output should have tanh activation\n",
        "* the output layer should have linear activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY4te2L8rthG"
      },
      "source": [
        "model = Sequential()\n",
        "<TODO>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: define early stopping compile the model, and train it\n",
        "Please pay attention to the following details:\n",
        "* use EarlyStopping with patience 10 and this time monitor the training loss \n",
        "* do not define validation data\n",
        "* use mean squarred error loss function\n",
        "* both the input and output of the model should be X this time. (as we are \"auto encoding\" the input to the output)"
      ],
      "metadata": {
        "id": "Qi_B1x4E-_8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patience = <TODO>\n",
        "es = EarlyStopping(<TODO>)\n",
        "model.compile(<TODO>)"
      ],
      "metadata": {
        "id": "YfgvoEnU-9B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oTvWs8PzZzj"
      },
      "source": [
        "history= model.fit(<TODO>)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2dyYrSFFCap"
      },
      "source": [
        "## Results analysis \n",
        "After training is done, we make predictions for the complete dataset and inspect the reconstruction loss. Our assumption is that, higher reconstruction loss are more likely to belong to anomalies than lower reconstruciton loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc11JrCKr4a2"
      },
      "source": [
        "preds = model.predict(X)\n",
        "errs  = np.square(preds-X)\n",
        "errs  = np.mean(errs,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgf6DVTmtR1D"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "rng = np.arange(len(errs))\n",
        "colors = ['g','r'] # green: normal, red: anomaly\n",
        "for i in [0,1]:\n",
        "    plt.scatter(rng[(Y==i).reshape(-1)], errs[(Y==i).reshape(-1)], color=colors[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLJuuLmdDTXd"
      },
      "source": [
        "Y_cut    = Y[errs<10]\n",
        "errs_cut = errs[errs<10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osgv-hhoDTYM"
      },
      "source": [
        "plt.figure(figsize=(16,10))\n",
        "rng = np.arange(len(errs_cut))\n",
        "colors = ['g','r'] \n",
        "for i in [0,1]:\n",
        "    plt.scatter(rng[(Y_cut==i).reshape(-1)], errs_cut[(Y_cut==i).reshape(-1)], color=colors[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLQnf_shFOfT"
      },
      "source": [
        "## Exercise 4: finding the right treshold\n",
        "Find a treshold that identifies at least 130 anomalies correctly with as small number of missclassifications of non-anomalies as possible. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X63H_qdQr72h"
      },
      "source": [
        "treshold = <TODO>\n",
        "errs_filtered               = errs[errs>treshold]\n",
        "Y_filtered                  = Y[errs>treshold,0]\n",
        "errs_filtered_non_anomalies = errs[errs<=treshold]\n",
        "Y_filtered_non_anomalies    = Y[errs<=treshold,0]\n",
        "\n",
        "print(\"Anomalies in the dataset\") # Y_filtered==1\n",
        "print(\"   predicted as anomaly:\", len(errs_filtered[Y_filtered==1]))\n",
        "print(\"   predicted as non-anomaly: \", len(errs_filtered_non_anomalies[Y_filtered_non_anomalies==1]) )\n",
        "print(\"Non-anomalies in the dataset\") # Y_filtered ==0\n",
        "print(\"   predicted as anomaly:\", len(errs_filtered[Y_filtered==0]))\n",
        "print(\"   predicted as non-anomaly:\", len(errs_filtered_non_anomalies[Y_filtered_non_anomalies==0]) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbDOf9DP1tjD"
      },
      "source": [
        "We can also inspect the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pwadetUDTYm"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "conf=confusion_matrix(Y, errs>treshold)\n",
        "import seaborn as sns\n",
        "sns.heatmap(conf, annot=True, vmax=8000, fmt=\"d\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}